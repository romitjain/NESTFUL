import pandas as pd
from pathlib import Path
import os, json, argparse, statistics, sys, signal, importlib
from tqdm import tqdm
from utils import *
from output_parsers import *
from sklearn.metrics import accuracy_score

def handler(signum, frame): 
    raise TimeoutError("Time limit exceeded!") 

DEEPSEEK = [
    "DeepSeek-V3",
    "DeepSeek-R1"
]

GRANITE_MODELS = [
    "granite-3.0-8b-instruct",
    "granite-3.0-8b-instruct-FT",
]

GRANITE_3_1_MODELS = [
    "granite-3.1-8b-instruct"
]

LLAMA_MODELS = [
    "Llama-3.1-8B-Instruct",
    "llama-3-1-70b-instruct",
    "llama-3-1-405b-instruct-fp8",
    "Llama-3.2-11B-Vision-Instruct",
    "Llama-3.2-90B-Vision-Instruct"
]

QWEN_MODELS = ["qwen-2.5-3B-instruct"]

def listit(t):
    return list(map(listit, t)) if isinstance(t, (list, tuple)) else t

def get_basic_func_list(basic_func_file_path):
    lines = read_file(basic_func_file_path)
    func_names = []
    for l in lines:
        if l.startswith("def "):
            func_str = l.strip().replace("def ","")
            func_name = func_str[:func_str.index('(')]
            func_names.append(func_name)
    return func_names

def calculate_ans(func_calls, spec_lib, executable_func_dir):
    signal.signal(signal.SIGALRM, handler) 
    signal.alarm(10)

    basic_func_file_name = "basic_functions.py"
    basic_func_file_path = os.path.join(executable_func_dir, basic_func_file_name)
    basic_func_list = get_basic_func_list(basic_func_file_path)
    func_file_dict = read_json(os.path.join(executable_func_dir, "func_file_map.json"))

    try:
        variable_result_map = {}
        for idx, f in enumerate(func_calls):
            label = f["label"].replace("$", "")
            output_params = [s for s in spec_lib if s["name"] == f["name"]][0]["output_parameters"]
            output_params = list(output_params.keys())
            arg_values = []
            arg_val_list = []
            for k, v in f["arguments"].items():
                if  type(v) == str and v.startswith("$") and v.endswith("$"):
                    v = v[1:-1]
                    v_l = v.split(".",1)[0]
                    out_param = v.split(".",1)[1]
                    v = variable_result_map[v_l][out_param]
                elif  type(v) == str and v.startswith("$var"):
                    v = v[1:]
                    v_l = v.split(".",1)[0]
                    out_param = v.split(".",1)[1]
                    v = variable_result_map[v_l][out_param]
                arg_val_list.append(v)
                arg_values.append(str(v))

            func_str = f"{f['name']}({','.join(arg_values)})"
            if f["name"] in basic_func_list:
                file_name = basic_func_file_name
            else:
                file_name = func_file_dict[f["name"]]
            file_path = os.path.join(executable_func_dir, file_name)

            spec = importlib.util.spec_from_file_location(file_name, file_path)
            mod = importlib.util.module_from_spec(spec)
            sys.modules[file_name] = mod
            spec.loader.exec_module(mod)
            func = getattr(mod, f['name'])
            try:
                res = func(*arg_val_list)
            except:
                return False

            if len(output_params) == 1:  
                variable_result_map[label] = {
                    output_params[0]: res
                }
            else:
                return False

        final_var = func_calls[-1]["label"].replace("$", "")
        final_ans = next(iter(variable_result_map[final_var].values()))
        return final_ans
    
    except TimeoutError: 
        print("The program timed out!") 
        signal.alarm(0)
        return False
    
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        print(exc_type, fname, exc_tb.tb_lineno)
        print(e)
        return False
    
def calculate_win_score(pred_func_calls, gold_ans, tools, executable_func_dir):
    if not pred_func_calls:
        return False
    
    gold_ans = json.loads(gold_ans)
    tools = json.loads(tools)
    pred_ans = calculate_ans(pred_func_calls, tools, executable_func_dir)
   
    if type(gold_ans) == float and type(pred_ans) == float:
        dec_no = len(str(gold_ans).split(".")[1])
        pred_ans = round(pred_ans, dec_no)
    if pred_ans == gold_ans:
        return True
    else:
        if type(pred_ans) == tuple and type(gold_ans) == list:
            pred_ans = listit(pred_ans)
            if pred_ans == gold_ans:
                return True
        return False


def get_alter_traj_scores(win_rate, acc_comb):
    print(len(win_rate))
    print(len(acc_comb))
    alt_traj_lst = []
    for wr, ac in zip(win_rate, acc_comb):
        if wr and ac < 1:
            alt_traj_lst.append(True)
        else:
            alt_traj_lst.append(False)
    sc = sum(alt_traj_lst) / len(alt_traj_lst)
    print(sc)
    return sc

def calculate_scores(predictions, model_name, executable_func_dir, intents_only=False, win_rate_flag=True, alt_traj_flag = True):
    gold_output_intent = []
    pred_output_intent = []
    gold_output_slot = []
    pred_output_slot = []
    p_intent, r_intent, f1_intent, p_slot, r_slot, f1_slot = None, None, None, None, None, None
    num_errors_parsing_pred_intent = 0
    num_errors_parsing_gold_intent = 0
    num_errors_parsing_pred_slot = 0
    num_errors_parsing_gold_slot = 0
    all_accuracy_combined = []
    all_num_times_full_score = 0
    win_rate_list = []

    num_pred_examples_w_parsing_errors = 0
    pred_examples_w_parsing_errors = []
    full_scores = []

    for item in tqdm(predictions):
        pred_has_parsing_errors = False
        pred_func_calls, gold_func_calls = [], []
        if model_name == 'Granite-20B-FunctionCalling':
            pred_func_calls, gold_func_calls, pred_dict_list, gold_dict_list, num_errors_parsing_pred_intent, pred_has_parsing_errors = parse_granite_20b_function_calling_output(item, num_errors_parsing_pred_intent)
        elif model_name == 'llama-3-70b-instruct' or model_name == "llama-3-1-405b-instruct":
            pred_func_calls, gold_func_calls, pred_dict_list, gold_dict_list, num_errors_parsing_pred_intent, pred_has_parsing_errors = parse_llama_3_70b_instruct(item, num_errors_parsing_pred_intent)
        elif model_name == 'Mistral-7B-Instruct-v0.3' or model_name == "mixtral_8x7b_instruct_v01" or model_name == "Mixtral-8x22B-Instruct-v0.1":
            pred_func_calls, gold_func_calls, pred_dict_list, gold_dict_list, num_errors_parsing_pred_intent, pred_has_parsing_errors = parse_mistral_7b_instruct_v0_3(item, num_errors_parsing_pred_intent)
        elif model_name == 'Hermes-2-Pro-Mistral-7B':
            pred_func_calls, gold_func_calls, pred_dict_list, gold_dict_list, num_errors_parsing_pred_intent, pred_has_parsing_errors = parse_hermes_2_pro_mistral_7B(item, num_errors_parsing_pred_intent)
        elif model_name in ['xLAM-1b-fc-r', "xLAM-7b-fc-r", "xLAM-8x7b-r", "xLAM-8x22b-r"]:
            pred_func_calls, gold_func_calls, pred_dict_list, gold_dict_list, num_errors_parsing_pred_intent, pred_has_parsing_errors = parse_xLAM_1b_fc_r(item, num_errors_parsing_pred_intent)
        elif model_name == 'Hammer2.0-7b':
            pred_func_calls, gold_func_calls, pred_dict_list, gold_dict_list, num_errors_parsing_pred_intent, pred_has_parsing_errors = parse_Hammer2_0_7b(item, num_errors_parsing_pred_intent)
        elif model_name == 'ToolAce-8b':
            pred_func_calls, gold_func_calls, pred_dict_list, gold_dict_list, num_errors_parsing_pred_intent, pred_has_parsing_errors = parse_ToolAce(item, num_errors_parsing_pred_intent)
        elif model_name in GRANITE_MODELS:
            pred_func_calls, gold_func_calls, pred_dict_list, gold_dict_list, num_errors_parsing_pred_intent, pred_has_parsing_errors = parse_granite_3_output(item, num_errors_parsing_pred_intent)
        elif model_name in GRANITE_3_1_MODELS + QWEN_MODELS:
            pred_func_calls, gold_func_calls, pred_dict_list, gold_dict_list, num_errors_parsing_pred_intent, pred_has_parsing_errors = parse_granite_3_1_output(item, num_errors_parsing_pred_intent)
        elif model_name in LLAMA_MODELS:
            pred_func_calls, gold_func_calls, pred_dict_list, gold_dict_list, num_errors_parsing_pred_intent, pred_has_parsing_errors = parse_llama_3_output(item, num_errors_parsing_pred_intent)
        elif model_name in DEEPSEEK:
            pred_func_calls, gold_func_calls, pred_dict_list, gold_dict_list, num_errors_parsing_pred_intent, pred_has_parsing_errors = parse_deepseek_output(item, num_errors_parsing_pred_intent)
        else:
            raise Exception("model not handled")

        gold_apis_names, pred_apis_names = [], []
        for f in pred_func_calls:
            if not f: continue
            try:
                if f.strip() == '{"name": "dummy", "arguments": {}}':
                    continue
                f = json.loads(f.replace('<|endoftext|>', '').strip())
                pred_apis_names.append(str(f['name']))
            except:
                num_errors_parsing_pred_intent += 1
                pred_has_parsing_errors = True
                pass
        for f in gold_func_calls:
            if not f: continue
            try:
                f = json.loads(f.replace('<|endoftext|>', '').strip())
                gold_apis_names.append(str(f['name']))
            except: 
                num_errors_parsing_gold_intent += 1
                pass
            
        gold_output_intent.append(gold_apis_names)
        pred_output_intent.append(pred_apis_names)
        if not intents_only:
            pred_api_map, gold_api_map = {}, {}
            for f in pred_func_calls:
                if f.strip() == '{"name": "dummy", "arguments": {}}':
                    continue
                try:
                    if not f: continue
                    f = json.loads(f.replace('<|endoftext|>', '').strip())
                    if type(f) != dict or 'name' not in f:
                        raise Exception
                    api_name = f['name']
                    pred_api_map[api_name] = []
                    for arg, val in f['arguments'].items():
                        if type(val) == str and val.startswith("$") and not val.endswith("$") :
                            val = val + "$"
                        pred_api_map[f['name']].append(f'{arg} = {val}')
                except:
                    num_errors_parsing_pred_slot += 1
                    pred_has_parsing_errors = True
                    pass
            for f in gold_func_calls:
                if not f: continue
                try:
                    f = json.loads(f.replace('<|endoftext|>', '').strip())
                    gold_api_map[f['name']] = []
                    for arg, val in f['arguments'].items():
                        gold_api_map[f['name']].append(f'{arg} = {val}')
                except: 
                    num_errors_parsing_gold_slot += 1
                    pass
            for key in set(pred_api_map.keys()).union(gold_api_map.keys()):
                if key in pred_api_map:
                    pred_output_slot.append(pred_api_map[key])
                else:
                    pred_output_slot.append([])
                if key in gold_api_map:
                    gold_output_slot.append(gold_api_map[key])
                else:
                    gold_output_slot.append([])

        if pred_has_parsing_errors:
            num_pred_examples_w_parsing_errors += 1
            pred_examples_w_parsing_errors.append(1)
        else:
            pred_examples_w_parsing_errors.append(0)

        api_with_args_gold = []
        for f in gold_func_calls:
            f = json.loads(f.replace('<|endoftext|>', '').strip())
            f_name = str(f['name'])
            args = ", ".join(sorted([f"{key} = {val}"  for key, val in f['arguments'].items()]))
            api_with_args_gold.append(f'{f_name}({args})')

        api_with_args_pred = []
        for f in pred_func_calls:
            try:
                f = json.loads(f.replace('<|endoftext|>', '').strip())
                f_name = str(f['name'])
                try:
                    arg_list = []
                    for key, val in f['arguments'].items():
                        if type(val) == str and val.startswith("$") and not val.endswith("$") :
                            val = val + "$"
                        arg_list.append(f"{key} = {val}")
                    args = ", ".join(sorted(arg_list))
                except:
                    args = {}
                api_with_args_pred.append(f'{f_name}({args})')
            except:
                continue
        api_with_args_gold, api_with_args_pred = post_process_api_with_args(api_with_args_gold, api_with_args_pred)

        try:
            accuracy_combined = accuracy_score(api_with_args_gold, api_with_args_pred)
        except:
            accuracy_combined = 0.0
        if accuracy_combined == 1:
            all_num_times_full_score += 1
            full_scores.append(1)
        else:
            full_scores.append(0)

        all_accuracy_combined.append(accuracy_combined)

        ## WinRate
        if win_rate_flag:
            win_score = calculate_win_score(pred_dict_list, item["gold_answer"], item["tools"], executable_func_dir)
            win_rate_list.append(win_score)

    p_intent, r_intent, f1_intent = compute_score_sklearn(gold_output_intent, pred_output_intent)
    p_slot, r_slot, f1_slot = compute_score_sklearn(gold_output_slot, pred_output_slot)

    # For final results DF
    df_data = {
        "sample_id": [p["sample_id"] for p in predictions],
        "pred_examples_w_parsing_errors": pred_examples_w_parsing_errors,
        "all_accuracy_combined": all_accuracy_combined,
        "full_scores": full_scores,
        "win_rate_list": win_rate_list
    }
    df = pd.DataFrame.from_dict(df_data)

    return {
        "p_intent": "{:.3f}".format(p_intent),
        "r_intent": "{:.3f}".format(r_intent),
        "f1_intent": "{:.3f}".format(f1_intent),
        "p_slot": "{:.3f}".format(p_slot) if p_slot != None else '',
        "r_slot": "{:.3f}".format(r_slot) if r_slot != None else '',
        "f1_slot": "{:.3f}".format(f1_slot) if f1_slot != None else '',
        'num_examples': len(predictions),
        'accuracy_combined': "{:.3f}".format(statistics.mean(all_accuracy_combined)),
        'percentage_times_full_score': "{:.3f}".format(all_num_times_full_score/len(predictions)),
        'win_rate': "{:.3f}".format(sum(win_rate_list) / len(win_rate_list)) if win_rate_flag else "no",
        'num_errors_parsing_pred_intent': num_errors_parsing_pred_intent,
        'num_errors_parsing_gold_intent': num_errors_parsing_gold_intent,
        'num_errors_parsing_pred_slot': num_errors_parsing_pred_slot,
        'num_errors_parsing_gold_slot': num_errors_parsing_gold_slot,
        'num_pred_examples_w_parsing_errors': num_pred_examples_w_parsing_errors
    }, df

def print_result(result, model):
    print("\n###################################")
    print(f"##### {model} #####")
    print("###################################")
    print(f"Total Samples: {result['num_examples']}")
    print(f"Parsing Errors: {result['num_pred_examples_w_parsing_errors']}")
    print(f"F1 Intent: {result['f1_intent']}")
    print(f"F1 Slot: {result['f1_slot']}")
    print(f"Partial Match Accuracy: {result['accuracy_combined']}")
    print(f"Full Match Accuracy: {result['percentage_times_full_score']}")
    print(f"Win Rate: {result['win_rate']}")
    print("-"*100)

def calculate_weighted_avg_score(res_lst):
    f1_intent_total = 0
    f1_slot_total = 0
    par_acc_total = 0
    full_acc_total = 0
    alt_traj_total = 0
    win_total = 0
    examples_total = 0
    parsing_errors_total = 0
    for res in res_lst:
        f1_intent_total += res["num_examples"] * float(res['f1_intent'])
        f1_slot_total += res["num_examples"] * float(res['f1_slot'])
        par_acc_total += res["num_examples"] * float(res['accuracy_combined'])
        full_acc_total += res["num_examples"] * float(res['percentage_times_full_score'])
        alt_traj_total += res["num_examples"] * float(res['alternate_trajectory_score'])
        win_total += res["num_examples"] * float(res['win_rate'])
        examples_total += res["num_examples"]
        parsing_errors_total += res["num_pred_examples_w_parsing_errors"]
    avg_res = {
        "num_examples": examples_total,
        "num_pred_examples_w_parsing_errors": parsing_errors_total, 
        "f1_intent": "{:.3f}".format(f1_intent_total / examples_total),
        "f1_slot": "{:.3f}".format(f1_slot_total / examples_total),
        "accuracy_combined": "{:.3f}".format(par_acc_total / examples_total),
        "percentage_times_full_score": "{:.3f}".format(full_acc_total / examples_total),
        "win_rate": "{:.3f}".format(win_total / examples_total),
        "alternate_trajectory_score": "{:.3f}".format(alt_traj_total / examples_total),
    }
    return avg_res
    
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", type=str)
    parser.add_argument("--result_file_path", type=str)
    parser.add_argument(
        "--executable_func_dir",
        type=str,
        required=False,
        default="data_v2/executable_functions",
    )
    args = parser.parse_args()

    data = read_jsonlines(args.result_file_path)
    result, result_df = calculate_scores(data, args.model_name, args.executable_func_dir)
    print_result(result, args.model_name)

    save_dir = str(Path(args.result_file_path).parent)
    print(f"Saving results to {save_dir}")

    result_df.to_csv(f"{save_dir}/results.csv", index=False)
    with open(f"{save_dir}/results.json", "w") as fp:
        json.dump(result, fp)

    test_ds = read_jsonlines("/cos-checkpoints/romit/data-mixing/data/odm/nestful_test.jsonl")
    test_ds = pd.DataFrame.from_records(test_ds)
    test_ds["category"] = test_ds.category.apply(lambda x: "advanced_math" if x == "geometry_problems" else x)

    test_ds = test_ds[["sample_id", "category", "order"]]
    out = test_ds.merge(result_df, left_on="sample_id", right_on="sample_id")

    cat_results = out.groupby(["category"]).agg(
        total=("category", "count"),
        parsing_err=("pred_examples_w_parsing_errors", lambda x: 100*round(x.sum()/len(x), 4)),
        partial_acc=("all_accuracy_combined", lambda x: 100*round(x.sum()/len(x), 4)),
        full_acc=("full_scores", lambda x: 100*round(x.sum()/len(x), 4)),
        win_rate=("win_rate_list", lambda x: 100*round(x.sum()/len(x), 4)),
    )

    print(cat_results)
    print(f"Overall: {100*round(sum(out.win_rate_list)/len(out), 4)}")

    cat_results.to_csv(f"{save_dir}/category_results.csv", index=False)
